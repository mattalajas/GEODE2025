{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(c_in, c_out)\n",
    "\n",
    "    def forward(self, node_feat, adj_mat):\n",
    "        num_neighbors = adj_mat.sum(dim = -1, keepdims=True)\n",
    "        node_feat = self.projection(node_feat)\n",
    "        node_feat = torch.bmm(adj_mat, node_feat)\n",
    "        node_feat = node_feat / num_neighbors\n",
    "        return node_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "nx_g = nx.from_edgelist([(2,1), (2,3), (4,2), (3,4)])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "nx.draw(nx_g, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1.],\n",
      "         [2., 3.],\n",
      "         [4., 5.],\n",
      "         [6., 7.]]])\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
    "\n",
    "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
    "                            [1, 1, 1, 1],\n",
    "                            [0, 1, 1, 1],\n",
    "                            [0, 1, 1, 1]]])\n",
    "\n",
    "print(node_feats)\n",
    "\n",
    "print(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.],\n",
      "         [4., 5.],\n",
      "         [4., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GCNLayer(c_in=2, c_out=2)\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix)\n",
    "\n",
    "print(out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, c_in, c_out, num_heads = 1, concat_heads = True, alpha = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat_heads = concat_heads\n",
    "\n",
    "        if self.concat_heads:\n",
    "            assert c_out % num_heads == 0\n",
    "            c_out = c_out // num_heads\n",
    "        \n",
    "        self.projection = nn.Linear(c_in, c_out*num_heads)\n",
    "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out))\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.projection.weight.data, gain = 1.414)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain = 1.414)\n",
    "\n",
    "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
    "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
    "\n",
    "        node_feats = self.projection(node_feats)\n",
    "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
    "\n",
    "        edges = adj_matrix.nonzero(as_tuple = False)\n",
    "\n",
    "        node_feats_flat = node_feats.view(batch_size*num_nodes, self.num_heads, -1)\n",
    "\n",
    "        edge_indices_row = edges[:, 0] * num_nodes + edges[:, 1]\n",
    "        edge_indices_col = edges[:, 0] * num_nodes + edges[:, 2]\n",
    "        a_input = torch.cat([\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim = 0),\n",
    "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim = 0)\n",
    "        ], dim = -1)\n",
    "\n",
    "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a)\n",
    "        attn_logits = self.leakyrelu(attn_logits)\n",
    "\n",
    "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
    "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
    "\n",
    "        attn_probs = F.softmax(attn_matrix, dim = 2)\n",
    "        if print_attn_probs:\n",
    "            print(attn_probs.permute(0, 3, 1, 2))\n",
    "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
    "\n",
    "        if self.concat_heads:\n",
    "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
    "        else:\n",
    "            node_feats = node_feats.mean(dim=2)\n",
    "\n",
    "        return node_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
      "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
      "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
      "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
      "\n",
      "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
      "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
      "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
      "tensor([[[1.2913, 1.9800],\n",
      "         [4.2344, 3.7725],\n",
      "         [4.6798, 4.8362],\n",
      "         [4.5043, 4.7351]]])\n"
     ]
    }
   ],
   "source": [
    "layer = GATLayer(c_in=2, c_out=2, num_heads=2)\n",
    "\n",
    "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
    "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
    "\n",
    "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_feats = layer(node_feats, adj_matrix, print_attn_probs = True)\n",
    "\n",
    "print(out_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as g_nn\n",
    "import torch_geometric.data as g_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.datasets\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = torch_geometric.datasets.Planetoid(root='./data', name='Cora', transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_chan):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1123)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_chan)\n",
    "        self.conv2 = GCNConv(hidden_chan, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_chan=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9459\n",
      "Epoch: 002, Loss: 1.9423\n",
      "Epoch: 003, Loss: 1.9363\n",
      "Epoch: 004, Loss: 1.9294\n",
      "Epoch: 005, Loss: 1.9196\n",
      "Epoch: 006, Loss: 1.9117\n",
      "Epoch: 007, Loss: 1.9036\n",
      "Epoch: 008, Loss: 1.8952\n",
      "Epoch: 009, Loss: 1.8858\n",
      "Epoch: 010, Loss: 1.8714\n",
      "Epoch: 011, Loss: 1.8623\n",
      "Epoch: 012, Loss: 1.8496\n",
      "Epoch: 013, Loss: 1.8320\n",
      "Epoch: 014, Loss: 1.8259\n",
      "Epoch: 015, Loss: 1.7992\n",
      "Epoch: 016, Loss: 1.8037\n",
      "Epoch: 017, Loss: 1.7824\n",
      "Epoch: 018, Loss: 1.7624\n",
      "Epoch: 019, Loss: 1.7550\n",
      "Epoch: 020, Loss: 1.7322\n",
      "Epoch: 021, Loss: 1.7187\n",
      "Epoch: 022, Loss: 1.7053\n",
      "Epoch: 023, Loss: 1.6713\n",
      "Epoch: 024, Loss: 1.6664\n",
      "Epoch: 025, Loss: 1.6434\n",
      "Epoch: 026, Loss: 1.6408\n",
      "Epoch: 027, Loss: 1.6100\n",
      "Epoch: 028, Loss: 1.5735\n",
      "Epoch: 029, Loss: 1.5686\n",
      "Epoch: 030, Loss: 1.5624\n",
      "Epoch: 031, Loss: 1.5369\n",
      "Epoch: 032, Loss: 1.5324\n",
      "Epoch: 033, Loss: 1.5009\n",
      "Epoch: 034, Loss: 1.4669\n",
      "Epoch: 035, Loss: 1.4342\n",
      "Epoch: 036, Loss: 1.4142\n",
      "Epoch: 037, Loss: 1.4200\n",
      "Epoch: 038, Loss: 1.3886\n",
      "Epoch: 039, Loss: 1.3605\n",
      "Epoch: 040, Loss: 1.3120\n",
      "Epoch: 041, Loss: 1.3290\n",
      "Epoch: 042, Loss: 1.2755\n",
      "Epoch: 043, Loss: 1.2718\n",
      "Epoch: 044, Loss: 1.2516\n",
      "Epoch: 045, Loss: 1.2151\n",
      "Epoch: 046, Loss: 1.2030\n",
      "Epoch: 047, Loss: 1.1951\n",
      "Epoch: 048, Loss: 1.1476\n",
      "Epoch: 049, Loss: 1.1865\n",
      "Epoch: 050, Loss: 1.1633\n",
      "Epoch: 051, Loss: 1.1051\n",
      "Epoch: 052, Loss: 1.1215\n",
      "Epoch: 053, Loss: 1.0710\n",
      "Epoch: 054, Loss: 1.0811\n",
      "Epoch: 055, Loss: 1.0303\n",
      "Epoch: 056, Loss: 1.0079\n",
      "Epoch: 057, Loss: 1.0166\n",
      "Epoch: 058, Loss: 0.9973\n",
      "Epoch: 059, Loss: 0.9860\n",
      "Epoch: 060, Loss: 0.9664\n",
      "Epoch: 061, Loss: 0.9455\n",
      "Epoch: 062, Loss: 0.9123\n",
      "Epoch: 063, Loss: 0.8985\n",
      "Epoch: 064, Loss: 0.8971\n",
      "Epoch: 065, Loss: 0.8645\n",
      "Epoch: 066, Loss: 0.8516\n",
      "Epoch: 067, Loss: 0.8384\n",
      "Epoch: 068, Loss: 0.8216\n",
      "Epoch: 069, Loss: 0.8440\n",
      "Epoch: 070, Loss: 0.8119\n",
      "Epoch: 071, Loss: 0.7746\n",
      "Epoch: 072, Loss: 0.8087\n",
      "Epoch: 073, Loss: 0.7953\n",
      "Epoch: 074, Loss: 0.7855\n",
      "Epoch: 075, Loss: 0.7669\n",
      "Epoch: 076, Loss: 0.7695\n",
      "Epoch: 077, Loss: 0.7377\n",
      "Epoch: 078, Loss: 0.7940\n",
      "Epoch: 079, Loss: 0.7259\n",
      "Epoch: 080, Loss: 0.7161\n",
      "Epoch: 081, Loss: 0.6819\n",
      "Epoch: 082, Loss: 0.6434\n",
      "Epoch: 083, Loss: 0.6627\n",
      "Epoch: 084, Loss: 0.6672\n",
      "Epoch: 085, Loss: 0.6425\n",
      "Epoch: 086, Loss: 0.6587\n",
      "Epoch: 087, Loss: 0.6332\n",
      "Epoch: 088, Loss: 0.6374\n",
      "Epoch: 089, Loss: 0.6179\n",
      "Epoch: 090, Loss: 0.6307\n",
      "Epoch: 091, Loss: 0.6213\n",
      "Epoch: 092, Loss: 0.5903\n",
      "Epoch: 093, Loss: 0.5946\n",
      "Epoch: 094, Loss: 0.5875\n",
      "Epoch: 095, Loss: 0.5927\n",
      "Epoch: 096, Loss: 0.5829\n",
      "Epoch: 097, Loss: 0.5739\n",
      "Epoch: 098, Loss: 0.5619\n",
      "Epoch: 099, Loss: 0.5295\n",
      "Epoch: 100, Loss: 0.5730\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_chan=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(model)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=1)\n",
      "  (conv2): GATConv(8, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_chan, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1123)\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_chan)\n",
    "        self.conv2 = GATConv(hidden_chan, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    test_correct = pred[mask] == data.y[mask]\n",
    "    test_acc = int(test_correct.sum()) / int(mask.sum())\n",
    "    return test_acc\n",
    "    \n",
    "gatmodel = GAT(hidden_chan=8, heads=8)\n",
    "optimizer = torch.optim.Adam(gatmodel.parameters(), lr = 0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print(gatmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.7602, Val; 0.7380,           Test: 0.7770\n",
      "Epoch: 002, Loss: 0.7905, Val; 0.7320,           Test: 0.7790\n",
      "Epoch: 003, Loss: 0.8240, Val; 0.7340,           Test: 0.7750\n",
      "Epoch: 004, Loss: 0.7399, Val; 0.7320,           Test: 0.7750\n",
      "Epoch: 005, Loss: 0.7183, Val; 0.7440,           Test: 0.7680\n",
      "Epoch: 006, Loss: 0.7952, Val; 0.7440,           Test: 0.7650\n",
      "Epoch: 007, Loss: 0.8133, Val; 0.7400,           Test: 0.7640\n",
      "Epoch: 008, Loss: 0.7635, Val; 0.7560,           Test: 0.7650\n",
      "Epoch: 009, Loss: 0.8283, Val; 0.7560,           Test: 0.7670\n",
      "Epoch: 010, Loss: 0.7206, Val; 0.7620,           Test: 0.7680\n",
      "Epoch: 011, Loss: 0.7232, Val; 0.7660,           Test: 0.7650\n",
      "Epoch: 012, Loss: 0.7161, Val; 0.7700,           Test: 0.7680\n",
      "Epoch: 013, Loss: 0.7365, Val; 0.7720,           Test: 0.7730\n",
      "Epoch: 014, Loss: 0.7093, Val; 0.7700,           Test: 0.7790\n",
      "Epoch: 015, Loss: 0.6774, Val; 0.7740,           Test: 0.7840\n",
      "Epoch: 016, Loss: 0.7981, Val; 0.7800,           Test: 0.7820\n",
      "Epoch: 017, Loss: 0.7350, Val; 0.7800,           Test: 0.7800\n",
      "Epoch: 018, Loss: 0.7277, Val; 0.7720,           Test: 0.7770\n",
      "Epoch: 019, Loss: 0.7120, Val; 0.7580,           Test: 0.7770\n",
      "Epoch: 020, Loss: 0.6932, Val; 0.7600,           Test: 0.7780\n",
      "Epoch: 021, Loss: 0.7185, Val; 0.7600,           Test: 0.7780\n",
      "Epoch: 022, Loss: 0.6533, Val; 0.7520,           Test: 0.7750\n",
      "Epoch: 023, Loss: 0.6856, Val; 0.7640,           Test: 0.7790\n",
      "Epoch: 024, Loss: 0.7011, Val; 0.7640,           Test: 0.7790\n",
      "Epoch: 025, Loss: 0.7041, Val; 0.7620,           Test: 0.7820\n",
      "Epoch: 026, Loss: 0.6337, Val; 0.7640,           Test: 0.7800\n",
      "Epoch: 027, Loss: 0.6456, Val; 0.7640,           Test: 0.7810\n",
      "Epoch: 028, Loss: 0.6507, Val; 0.7600,           Test: 0.7730\n",
      "Epoch: 029, Loss: 0.7917, Val; 0.7620,           Test: 0.7700\n",
      "Epoch: 030, Loss: 0.6299, Val; 0.7600,           Test: 0.7720\n",
      "Epoch: 031, Loss: 0.6147, Val; 0.7620,           Test: 0.7740\n",
      "Epoch: 032, Loss: 0.6257, Val; 0.7640,           Test: 0.7740\n",
      "Epoch: 033, Loss: 0.6234, Val; 0.7600,           Test: 0.7750\n",
      "Epoch: 034, Loss: 0.6361, Val; 0.7600,           Test: 0.7750\n",
      "Epoch: 035, Loss: 0.6814, Val; 0.7600,           Test: 0.7760\n",
      "Epoch: 036, Loss: 0.7038, Val; 0.7620,           Test: 0.7760\n",
      "Epoch: 037, Loss: 0.6486, Val; 0.7680,           Test: 0.7790\n",
      "Epoch: 038, Loss: 0.5665, Val; 0.7700,           Test: 0.7760\n",
      "Epoch: 039, Loss: 0.6898, Val; 0.7700,           Test: 0.7860\n",
      "Epoch: 040, Loss: 0.6299, Val; 0.7640,           Test: 0.7900\n",
      "Epoch: 041, Loss: 0.6275, Val; 0.7620,           Test: 0.7840\n",
      "Epoch: 042, Loss: 0.6535, Val; 0.7540,           Test: 0.7830\n",
      "Epoch: 043, Loss: 0.6120, Val; 0.7520,           Test: 0.7820\n",
      "Epoch: 044, Loss: 0.6611, Val; 0.7600,           Test: 0.7820\n",
      "Epoch: 045, Loss: 0.6965, Val; 0.7620,           Test: 0.7870\n",
      "Epoch: 046, Loss: 0.6554, Val; 0.7720,           Test: 0.7880\n",
      "Epoch: 047, Loss: 0.5904, Val; 0.7740,           Test: 0.7910\n",
      "Epoch: 048, Loss: 0.6743, Val; 0.7620,           Test: 0.7880\n",
      "Epoch: 049, Loss: 0.6894, Val; 0.7560,           Test: 0.7800\n",
      "Epoch: 050, Loss: 0.5616, Val; 0.7520,           Test: 0.7770\n",
      "Epoch: 051, Loss: 0.6670, Val; 0.7540,           Test: 0.7780\n",
      "Epoch: 052, Loss: 0.6301, Val; 0.7520,           Test: 0.7760\n",
      "Epoch: 053, Loss: 0.5910, Val; 0.7460,           Test: 0.7750\n",
      "Epoch: 054, Loss: 0.5302, Val; 0.7480,           Test: 0.7740\n",
      "Epoch: 055, Loss: 0.6196, Val; 0.7500,           Test: 0.7730\n",
      "Epoch: 056, Loss: 0.6289, Val; 0.7500,           Test: 0.7740\n",
      "Epoch: 057, Loss: 0.5859, Val; 0.7520,           Test: 0.7770\n",
      "Epoch: 058, Loss: 0.5369, Val; 0.7560,           Test: 0.7820\n",
      "Epoch: 059, Loss: 0.5929, Val; 0.7560,           Test: 0.7830\n",
      "Epoch: 060, Loss: 0.5033, Val; 0.7640,           Test: 0.7840\n",
      "Epoch: 061, Loss: 0.5463, Val; 0.7580,           Test: 0.7830\n",
      "Epoch: 062, Loss: 0.5772, Val; 0.7520,           Test: 0.7780\n",
      "Epoch: 063, Loss: 0.5887, Val; 0.7540,           Test: 0.7750\n",
      "Epoch: 064, Loss: 0.5997, Val; 0.7520,           Test: 0.7760\n",
      "Epoch: 065, Loss: 0.5994, Val; 0.7460,           Test: 0.7740\n",
      "Epoch: 066, Loss: 0.4745, Val; 0.7480,           Test: 0.7730\n",
      "Epoch: 067, Loss: 0.6160, Val; 0.7420,           Test: 0.7710\n",
      "Epoch: 068, Loss: 0.5279, Val; 0.7440,           Test: 0.7700\n",
      "Epoch: 069, Loss: 0.5894, Val; 0.7380,           Test: 0.7690\n",
      "Epoch: 070, Loss: 0.5201, Val; 0.7460,           Test: 0.7700\n",
      "Epoch: 071, Loss: 0.5487, Val; 0.7560,           Test: 0.7820\n",
      "Epoch: 072, Loss: 0.4845, Val; 0.7600,           Test: 0.7880\n",
      "Epoch: 073, Loss: 0.5394, Val; 0.7680,           Test: 0.7960\n",
      "Epoch: 074, Loss: 0.4971, Val; 0.7760,           Test: 0.7940\n",
      "Epoch: 075, Loss: 0.5836, Val; 0.7780,           Test: 0.8000\n",
      "Epoch: 076, Loss: 0.5255, Val; 0.7840,           Test: 0.8060\n",
      "Epoch: 077, Loss: 0.5658, Val; 0.7840,           Test: 0.8020\n",
      "Epoch: 078, Loss: 0.5053, Val; 0.7860,           Test: 0.8000\n",
      "Epoch: 079, Loss: 0.6352, Val; 0.7820,           Test: 0.7970\n",
      "Epoch: 080, Loss: 0.5482, Val; 0.7800,           Test: 0.7950\n",
      "Epoch: 081, Loss: 0.5521, Val; 0.7780,           Test: 0.7940\n",
      "Epoch: 082, Loss: 0.5146, Val; 0.7780,           Test: 0.7920\n",
      "Epoch: 083, Loss: 0.5553, Val; 0.7760,           Test: 0.7920\n",
      "Epoch: 084, Loss: 0.5635, Val; 0.7700,           Test: 0.7910\n",
      "Epoch: 085, Loss: 0.4820, Val; 0.7720,           Test: 0.7900\n",
      "Epoch: 086, Loss: 0.4853, Val; 0.7720,           Test: 0.7970\n",
      "Epoch: 087, Loss: 0.5080, Val; 0.7720,           Test: 0.8010\n",
      "Epoch: 088, Loss: 0.4912, Val; 0.7660,           Test: 0.7990\n",
      "Epoch: 089, Loss: 0.4620, Val; 0.7660,           Test: 0.7950\n",
      "Epoch: 090, Loss: 0.5628, Val; 0.7640,           Test: 0.7940\n",
      "Epoch: 091, Loss: 0.5450, Val; 0.7620,           Test: 0.7900\n",
      "Epoch: 092, Loss: 0.4989, Val; 0.7620,           Test: 0.7870\n",
      "Epoch: 093, Loss: 0.6359, Val; 0.7580,           Test: 0.7820\n",
      "Epoch: 094, Loss: 0.5099, Val; 0.7580,           Test: 0.7830\n",
      "Epoch: 095, Loss: 0.4949, Val; 0.7620,           Test: 0.7860\n",
      "Epoch: 096, Loss: 0.5520, Val; 0.7640,           Test: 0.7850\n",
      "Epoch: 097, Loss: 0.4932, Val; 0.7680,           Test: 0.7870\n",
      "Epoch: 098, Loss: 0.5356, Val; 0.7640,           Test: 0.7870\n",
      "Epoch: 099, Loss: 0.5020, Val; 0.7660,           Test: 0.7790\n",
      "Epoch: 100, Loss: 0.4990, Val; 0.7720,           Test: 0.7800\n",
      "Epoch: 101, Loss: 0.5163, Val; 0.7760,           Test: 0.7790\n",
      "Epoch: 102, Loss: 0.4503, Val; 0.7820,           Test: 0.7820\n",
      "Epoch: 103, Loss: 0.4806, Val; 0.7820,           Test: 0.7870\n",
      "Epoch: 104, Loss: 0.4624, Val; 0.7780,           Test: 0.7920\n",
      "Epoch: 105, Loss: 0.4850, Val; 0.7840,           Test: 0.8020\n",
      "Epoch: 106, Loss: 0.5060, Val; 0.7860,           Test: 0.8050\n",
      "Epoch: 107, Loss: 0.5129, Val; 0.7800,           Test: 0.8030\n",
      "Epoch: 108, Loss: 0.4709, Val; 0.7780,           Test: 0.7960\n",
      "Epoch: 109, Loss: 0.5023, Val; 0.7800,           Test: 0.7920\n",
      "Epoch: 110, Loss: 0.5736, Val; 0.7800,           Test: 0.7920\n",
      "Epoch: 111, Loss: 0.5550, Val; 0.7820,           Test: 0.7890\n",
      "Epoch: 112, Loss: 0.4895, Val; 0.7800,           Test: 0.7890\n",
      "Epoch: 113, Loss: 0.4979, Val; 0.7800,           Test: 0.7930\n",
      "Epoch: 114, Loss: 0.4965, Val; 0.7800,           Test: 0.7930\n",
      "Epoch: 115, Loss: 0.5976, Val; 0.7780,           Test: 0.7920\n",
      "Epoch: 116, Loss: 0.5026, Val; 0.7780,           Test: 0.7880\n",
      "Epoch: 117, Loss: 0.5072, Val; 0.7780,           Test: 0.7900\n",
      "Epoch: 118, Loss: 0.5446, Val; 0.7700,           Test: 0.7820\n",
      "Epoch: 119, Loss: 0.4376, Val; 0.7720,           Test: 0.7800\n",
      "Epoch: 120, Loss: 0.5151, Val; 0.7720,           Test: 0.7800\n",
      "Epoch: 121, Loss: 0.4108, Val; 0.7700,           Test: 0.7800\n",
      "Epoch: 122, Loss: 0.4243, Val; 0.7740,           Test: 0.7830\n",
      "Epoch: 123, Loss: 0.4730, Val; 0.7780,           Test: 0.7870\n",
      "Epoch: 124, Loss: 0.4533, Val; 0.7820,           Test: 0.7900\n",
      "Epoch: 125, Loss: 0.4522, Val; 0.7800,           Test: 0.7950\n",
      "Epoch: 126, Loss: 0.5703, Val; 0.7820,           Test: 0.7980\n",
      "Epoch: 127, Loss: 0.4290, Val; 0.7840,           Test: 0.8000\n",
      "Epoch: 128, Loss: 0.4677, Val; 0.7840,           Test: 0.8030\n",
      "Epoch: 129, Loss: 0.5560, Val; 0.7860,           Test: 0.8040\n",
      "Epoch: 130, Loss: 0.5428, Val; 0.7820,           Test: 0.8010\n",
      "Epoch: 131, Loss: 0.4999, Val; 0.7820,           Test: 0.8020\n",
      "Epoch: 132, Loss: 0.4895, Val; 0.7820,           Test: 0.7970\n",
      "Epoch: 133, Loss: 0.5150, Val; 0.7820,           Test: 0.7960\n",
      "Epoch: 134, Loss: 0.4762, Val; 0.7780,           Test: 0.7970\n",
      "Epoch: 135, Loss: 0.5097, Val; 0.7800,           Test: 0.8000\n",
      "Epoch: 136, Loss: 0.5279, Val; 0.7800,           Test: 0.7990\n",
      "Epoch: 137, Loss: 0.4665, Val; 0.7760,           Test: 0.7950\n",
      "Epoch: 138, Loss: 0.4917, Val; 0.7780,           Test: 0.7940\n",
      "Epoch: 139, Loss: 0.4907, Val; 0.7740,           Test: 0.7930\n",
      "Epoch: 140, Loss: 0.4159, Val; 0.7760,           Test: 0.7950\n",
      "Epoch: 141, Loss: 0.4887, Val; 0.7800,           Test: 0.7940\n",
      "Epoch: 142, Loss: 0.4281, Val; 0.7800,           Test: 0.7930\n",
      "Epoch: 143, Loss: 0.4843, Val; 0.7800,           Test: 0.7890\n",
      "Epoch: 144, Loss: 0.4388, Val; 0.7760,           Test: 0.7920\n",
      "Epoch: 145, Loss: 0.5349, Val; 0.7760,           Test: 0.7900\n",
      "Epoch: 146, Loss: 0.4575, Val; 0.7820,           Test: 0.7910\n",
      "Epoch: 147, Loss: 0.4208, Val; 0.7780,           Test: 0.7920\n",
      "Epoch: 148, Loss: 0.5589, Val; 0.7740,           Test: 0.7940\n",
      "Epoch: 149, Loss: 0.4332, Val; 0.7820,           Test: 0.7960\n",
      "Epoch: 150, Loss: 0.4161, Val; 0.7780,           Test: 0.7980\n",
      "Epoch: 151, Loss: 0.4349, Val; 0.7760,           Test: 0.7970\n",
      "Epoch: 152, Loss: 0.4368, Val; 0.7800,           Test: 0.7970\n",
      "Epoch: 153, Loss: 0.4195, Val; 0.7860,           Test: 0.7970\n",
      "Epoch: 154, Loss: 0.4694, Val; 0.7880,           Test: 0.7980\n",
      "Epoch: 155, Loss: 0.5086, Val; 0.7900,           Test: 0.8010\n",
      "Epoch: 156, Loss: 0.4533, Val; 0.7820,           Test: 0.8020\n",
      "Epoch: 157, Loss: 0.5370, Val; 0.7820,           Test: 0.8020\n",
      "Epoch: 158, Loss: 0.4483, Val; 0.7880,           Test: 0.8050\n",
      "Epoch: 159, Loss: 0.4568, Val; 0.7860,           Test: 0.8030\n",
      "Epoch: 160, Loss: 0.5263, Val; 0.7820,           Test: 0.7960\n",
      "Epoch: 161, Loss: 0.4729, Val; 0.7820,           Test: 0.7940\n",
      "Epoch: 162, Loss: 0.4455, Val; 0.7800,           Test: 0.7950\n",
      "Epoch: 163, Loss: 0.4938, Val; 0.7760,           Test: 0.7900\n",
      "Epoch: 164, Loss: 0.3815, Val; 0.7740,           Test: 0.7900\n",
      "Epoch: 165, Loss: 0.4717, Val; 0.7780,           Test: 0.7880\n",
      "Epoch: 166, Loss: 0.4421, Val; 0.7760,           Test: 0.7890\n",
      "Epoch: 167, Loss: 0.4289, Val; 0.7800,           Test: 0.7900\n",
      "Epoch: 168, Loss: 0.4992, Val; 0.7780,           Test: 0.7910\n",
      "Epoch: 169, Loss: 0.4563, Val; 0.7800,           Test: 0.7950\n",
      "Epoch: 170, Loss: 0.4393, Val; 0.7820,           Test: 0.7950\n",
      "Epoch: 171, Loss: 0.4239, Val; 0.7860,           Test: 0.7980\n",
      "Epoch: 172, Loss: 0.4723, Val; 0.7880,           Test: 0.8000\n",
      "Epoch: 173, Loss: 0.5252, Val; 0.7900,           Test: 0.8000\n",
      "Epoch: 174, Loss: 0.3903, Val; 0.7860,           Test: 0.8060\n",
      "Epoch: 175, Loss: 0.4997, Val; 0.7860,           Test: 0.8010\n",
      "Epoch: 176, Loss: 0.4758, Val; 0.7860,           Test: 0.7990\n",
      "Epoch: 177, Loss: 0.4237, Val; 0.7840,           Test: 0.7980\n",
      "Epoch: 178, Loss: 0.3868, Val; 0.7860,           Test: 0.7990\n",
      "Epoch: 179, Loss: 0.4833, Val; 0.7800,           Test: 0.7980\n",
      "Epoch: 180, Loss: 0.4678, Val; 0.7820,           Test: 0.7970\n",
      "Epoch: 181, Loss: 0.4726, Val; 0.7820,           Test: 0.7970\n",
      "Epoch: 182, Loss: 0.4703, Val; 0.7820,           Test: 0.7970\n",
      "Epoch: 183, Loss: 0.4094, Val; 0.7860,           Test: 0.8010\n",
      "Epoch: 184, Loss: 0.3754, Val; 0.7860,           Test: 0.8030\n",
      "Epoch: 185, Loss: 0.4006, Val; 0.7880,           Test: 0.8070\n",
      "Epoch: 186, Loss: 0.4247, Val; 0.7860,           Test: 0.8040\n",
      "Epoch: 187, Loss: 0.4344, Val; 0.7880,           Test: 0.7990\n",
      "Epoch: 188, Loss: 0.4193, Val; 0.7880,           Test: 0.7980\n",
      "Epoch: 189, Loss: 0.4478, Val; 0.7800,           Test: 0.7980\n",
      "Epoch: 190, Loss: 0.4753, Val; 0.7780,           Test: 0.7990\n",
      "Epoch: 191, Loss: 0.4261, Val; 0.7800,           Test: 0.7990\n",
      "Epoch: 192, Loss: 0.4408, Val; 0.7820,           Test: 0.7990\n",
      "Epoch: 193, Loss: 0.4747, Val; 0.7840,           Test: 0.7990\n",
      "Epoch: 194, Loss: 0.4034, Val; 0.7860,           Test: 0.8050\n",
      "Epoch: 195, Loss: 0.4725, Val; 0.7860,           Test: 0.7990\n",
      "Epoch: 196, Loss: 0.4515, Val; 0.7840,           Test: 0.7980\n",
      "Epoch: 197, Loss: 0.3532, Val; 0.7860,           Test: 0.7960\n",
      "Epoch: 198, Loss: 0.4104, Val; 0.7840,           Test: 0.7970\n",
      "Epoch: 199, Loss: 0.4609, Val; 0.7840,           Test: 0.7950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 200):\n",
    "    loss = train(gatmodel)\n",
    "    val_acc = test(gatmodel, data.val_mask)\n",
    "    test_acc = test(gatmodel, data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val; {val_acc:.4f}, \\\n",
    "          Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_acc = test(gatmodel, mask=data.train_mask)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

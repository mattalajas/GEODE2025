{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tsl import logger\n",
    "from tsl.data import ImputationDataset, SpatioTemporalDataModule\n",
    "from tsl.data.preprocessing import StandardScaler\n",
    "from tsl.datasets import AirQuality, MetrLA, PemsBay\n",
    "from tsl.engines import Imputer\n",
    "from tsl.experiment import Experiment\n",
    "from tsl.metrics import numpy as numpy_metrics\n",
    "from tsl.metrics import torch as torch_metrics\n",
    "from tsl.nn.models import (BiRNNImputerModel, GRINModel, RNNImputerModel,\n",
    "                           SPINHierarchicalModel, SPINModel)\n",
    "from tsl.ops.imputation import add_missing_values\n",
    "from tsl.transforms import MaskInput\n",
    "from tsl.utils.casting import torch_to_numpy\n",
    "\n",
    "\n",
    "def get_model_class(model_str):\n",
    "    if model_str == 'rnni':\n",
    "        model = RNNImputerModel\n",
    "    elif model_str == 'birnni':\n",
    "        model = BiRNNImputerModel\n",
    "    elif model_str == 'grin':\n",
    "        model = GRINModel\n",
    "    elif model_str == 'spin':\n",
    "        model = SPINModel\n",
    "    elif model_str == 'spin-h':\n",
    "        model = SPINHierarchicalModel\n",
    "    else:\n",
    "        raise NotImplementedError(f'Model \"{model_str}\" not available.')\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_dataset(dataset_name: str, p_fault=0., p_noise=0.):\n",
    "    if dataset_name.startswith('air'):\n",
    "        return AirQuality(impute_nans=True, small=True)\n",
    "    if dataset_name.endswith('_point'):\n",
    "        p_fault, p_noise = 0., 0.25\n",
    "        dataset_name = dataset_name[:-6]\n",
    "    if dataset_name.endswith('_block'):\n",
    "        p_fault, p_noise = 0.0015, 0.05\n",
    "        dataset_name = dataset_name[:-6]\n",
    "    if dataset_name == 'la':\n",
    "        return add_missing_values(MetrLA(),\n",
    "                                  p_fault=p_fault,\n",
    "                                  p_noise=p_noise,\n",
    "                                  min_seq=12,\n",
    "                                  max_seq=12 * 4,\n",
    "                                  seed=9101112)\n",
    "    if dataset_name == 'bay':\n",
    "        return add_missing_values(PemsBay(),\n",
    "                                  p_fault=p_fault,\n",
    "                                  p_noise=p_noise,\n",
    "                                  min_seq=12,\n",
    "                                  max_seq=12 * 4,\n",
    "                                  seed=56789)\n",
    "    raise ValueError(f\"Dataset {dataset_name} not available in this setting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = np.random.random\n",
    "randint = np.random.randint\n",
    "shape = (32, 12, 6, 1)\n",
    "\n",
    "mask = np.zeros(shape).astype(bool)\n",
    "road_shape = mask.shape[1]\n",
    "rand_mask = rand(road_shape) < 0.5\n",
    "road_mask = np.zeros(shape).astype(bool)\n",
    "road_mask[:, rand_mask] = True\n",
    "mask = mask | road_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]],\n",
       "\n",
       "\n",
       "       [[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]],\n",
       "\n",
       "\n",
       "       [[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]],\n",
       "\n",
       "\n",
       "       [[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]],\n",
       "\n",
       "\n",
       "       [[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True],\n",
       "         [ True]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "192\n",
      "192\n",
      "192\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(12):\n",
    "    print(np.sum(mask[:, i, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset('air')\n",
    "\n",
    "# encode time of the day and use it as exogenous variable\n",
    "covariates = {'u': dataset.datetime_encoded('day').values}\n",
    "\n",
    "# get adjacency matrix\n",
    "adj = dataset.get_connectivity('distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8759, 36)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8675299057477388"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset.mask)/315324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th>1001</th>\n",
       "      <th>1002</th>\n",
       "      <th>1003</th>\n",
       "      <th>1004</th>\n",
       "      <th>1005</th>\n",
       "      <th>1006</th>\n",
       "      <th>1007</th>\n",
       "      <th>1008</th>\n",
       "      <th>1009</th>\n",
       "      <th>1010</th>\n",
       "      <th>...</th>\n",
       "      <th>1027</th>\n",
       "      <th>1028</th>\n",
       "      <th>1029</th>\n",
       "      <th>1030</th>\n",
       "      <th>1031</th>\n",
       "      <th>1032</th>\n",
       "      <th>1033</th>\n",
       "      <th>1034</th>\n",
       "      <th>1035</th>\n",
       "      <th>1036</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-05-01 01:00:00</th>\n",
       "      <td>138.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>21.333334</td>\n",
       "      <td>97.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01 02:00:00</th>\n",
       "      <td>124.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>97.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01 03:00:00</th>\n",
       "      <td>127.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>...</td>\n",
       "      <td>103.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>103.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01 04:00:00</th>\n",
       "      <td>129.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>107.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-01 05:00:00</th>\n",
       "      <td>119.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>111.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30 19:00:00</th>\n",
       "      <td>86.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>74.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30 20:00:00</th>\n",
       "      <td>87.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30 21:00:00</th>\n",
       "      <td>80.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30 22:00:00</th>\n",
       "      <td>98.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>95.5</td>\n",
       "      <td>90.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>93.5</td>\n",
       "      <td>...</td>\n",
       "      <td>178.0</td>\n",
       "      <td>99.5</td>\n",
       "      <td>94.0</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>82.5</td>\n",
       "      <td>84.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-30 23:00:00</th>\n",
       "      <td>79.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>208.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8759 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes                 1001   1002   1003   1004   1005   1006   1007   1008  \\\n",
       "channels                 0      0      0      0      0      0      0      0   \n",
       "datetime                                                                      \n",
       "2014-05-01 01:00:00  138.0   89.0  105.0   98.0  109.0   87.0   88.0   91.0   \n",
       "2014-05-01 02:00:00  124.0   85.0  121.0  107.0  101.0   99.0  105.0  102.0   \n",
       "2014-05-01 03:00:00  127.0   88.0  130.0  115.0  102.0  109.0  114.0  108.0   \n",
       "2014-05-01 04:00:00  129.0  100.0  137.0  123.0  108.0  118.0  118.0  109.0   \n",
       "2014-05-01 05:00:00  119.0  109.0  144.0  129.0  115.0  124.0  130.0  116.0   \n",
       "...                    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "2015-04-30 19:00:00   86.0   72.0   70.0   72.0   73.0   63.0   65.0   69.0   \n",
       "2015-04-30 20:00:00   87.0   73.0   73.0   76.0   83.0   63.0   61.0   62.0   \n",
       "2015-04-30 21:00:00   80.0   74.0   80.0   70.0   84.0   69.0   72.0   75.0   \n",
       "2015-04-30 22:00:00   98.0   87.5   87.0   95.5   90.5   76.0   79.0   83.0   \n",
       "2015-04-30 23:00:00   79.0   73.0   93.0   77.0   80.0   73.0  131.0   74.0   \n",
       "\n",
       "nodes                 1009   1010  ...   1027   1028   1029       1030   1031  \\\n",
       "channels                 0      0  ...      0      0      0          0      0   \n",
       "datetime                           ...                                          \n",
       "2014-05-01 01:00:00   87.0   87.0  ...  101.0   84.0  117.0  21.333334   97.0   \n",
       "2014-05-01 02:00:00  103.0   94.0  ...  100.0   77.0  109.0  78.000000   97.0   \n",
       "2014-05-01 03:00:00  112.0  109.0  ...  103.0   90.0  105.0  77.000000  103.0   \n",
       "2014-05-01 04:00:00  117.0  111.0  ...  110.0   94.0  105.0  90.000000  107.0   \n",
       "2014-05-01 05:00:00  124.0  114.0  ...  105.0   80.0  104.0  83.000000  111.0   \n",
       "...                    ...    ...  ...    ...    ...    ...        ...    ...   \n",
       "2015-04-30 19:00:00   74.0   66.0  ...  130.0  136.0   79.0  69.000000   74.0   \n",
       "2015-04-30 20:00:00   62.0   68.0  ...  133.0  124.0   64.0  67.000000   75.0   \n",
       "2015-04-30 21:00:00   77.0   73.0  ...  117.0  123.0   72.0  70.000000   82.0   \n",
       "2015-04-30 22:00:00   82.0   93.5  ...  178.0   99.5   94.0  74.000000   99.0   \n",
       "2015-04-30 23:00:00   82.0   94.0  ...  208.0  178.0   79.0  87.000000   80.0   \n",
       "\n",
       "nodes                 1032   1033   1034   1035   1036  \n",
       "channels                 0      0      0      0      0  \n",
       "datetime                                                \n",
       "2014-05-01 01:00:00   87.0   74.0   94.0  112.0  109.0  \n",
       "2014-05-01 02:00:00   84.0   84.0  101.0  123.0  114.0  \n",
       "2014-05-01 03:00:00   83.0  100.0  112.0  143.0  126.0  \n",
       "2014-05-01 04:00:00   88.0  103.0  120.0  138.0  130.0  \n",
       "2014-05-01 05:00:00   85.0  108.0  125.0  145.0  137.0  \n",
       "...                    ...    ...    ...    ...    ...  \n",
       "2015-04-30 19:00:00  125.0   94.0   67.0   64.0   79.0  \n",
       "2015-04-30 20:00:00  124.0  128.0  103.0   69.0   77.0  \n",
       "2015-04-30 21:00:00  125.0  139.0  166.0   78.0   94.0  \n",
       "2015-04-30 22:00:00   82.5   84.0  103.0   86.0   91.5  \n",
       "2015-04-30 23:00:00  147.0  164.0  244.0   94.0   88.0  \n",
       "\n",
       "[8759 rows x 36 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.        ,   10.01553231,   19.62147943, ..., 1903.71716932,\n",
       "        1796.30609352, 1795.96589939],\n",
       "       [  10.01553231,    0.        ,   10.11719073, ..., 1895.15226463,\n",
       "        1787.72195802, 1787.39244677],\n",
       "       [  19.62147943,   10.11719073,    0.        , ..., 1885.08052065,\n",
       "        1777.64679953, 1777.31922415],\n",
       "       ...,\n",
       "       [1903.71716932, 1895.15226463, 1885.08052065, ...,    0.        ,\n",
       "         107.63326768,  107.79738345],\n",
       "       [1796.30609352, 1787.72195802, 1777.64679953, ...,  107.63326768,\n",
       "           0.        ,    3.6701503 ],\n",
       "       [1795.96589939, 1787.39244677, 1777.31922415, ...,  107.79738345,\n",
       "           3.6701503 ,    0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8759, 1, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(covariates['u'], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8759, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates['u'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "beijing_dataset = pd.read_csv('../../../AirData/AQI/Stations/merged_full.csv')\n",
    "beijing_dataset['datetime'] = pd.to_datetime(beijing_dataset[['year', 'month', 'day', 'hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = beijing_dataset.drop_duplicates(subset=[\"station\"])[[\"station\", \"locationLatitude\", \"locationLongitude\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_coord = stations.loc[:, ['locationLatitude', 'locationLongitude']]\n",
    "from tsl.ops.similarities import geographical_distance\n",
    "dist = geographical_distance(st_coord, to_rad=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = beijing_dataset.pivot(index=\"datetime\", columns=\"station\", values=[\"PM2.5\", \"TEMP\"])\n",
    "df_pivot.columns.names = [\"channels\", \"nodes\"]\n",
    "df_pivot.columns = df_pivot.columns.swaplevel(0, 1) \n",
    "df_pivot.sort_index(axis=1, level=0, inplace=True)\n",
    "df_pivot = df_pivot.rename(columns={'PM2.5': 0, 'TEMP': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Aotizhongxin</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Changping</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dingling</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Dongsi</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Guanyuan</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Nongzhanguan</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Shunyi</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Tiantan</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Wanliu</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Wanshouxigong</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-03-01 00:00:00</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 01:00:00</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 02:00:00</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 03:00:00</th>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01 04:00:00</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28 19:00:00</th>\n",
       "      <td>12.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28 20:00:00</th>\n",
       "      <td>13.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>47.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28 21:00:00</th>\n",
       "      <td>16.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28 22:00:00</th>\n",
       "      <td>21.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-28 23:00:00</th>\n",
       "      <td>19.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35064 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes               Aotizhongxin       Changping       Dingling       Dongsi  \\\n",
       "channels                       0     1         0     1        0     1      0   \n",
       "datetime                                                                       \n",
       "2013-03-01 00:00:00          4.0  -0.7       3.0  -2.3      4.0  -2.3    9.0   \n",
       "2013-03-01 01:00:00          8.0  -1.1       3.0  -2.5      7.0  -2.5    4.0   \n",
       "2013-03-01 02:00:00          7.0  -1.1       3.0  -3.0      5.0  -3.0    7.0   \n",
       "2013-03-01 03:00:00          6.0  -1.4       3.0  -3.6      6.0  -3.6    3.0   \n",
       "2013-03-01 04:00:00          3.0  -2.0       3.0  -3.5      5.0  -3.5    3.0   \n",
       "...                          ...   ...       ...   ...      ...   ...    ...   \n",
       "2017-02-28 19:00:00         12.0  12.5      28.0  11.7     11.0  11.7   16.0   \n",
       "2017-02-28 20:00:00         13.0  11.6      12.0  10.9     13.0  10.9   18.0   \n",
       "2017-02-28 21:00:00         16.0  10.8       7.0   9.5      9.0   9.5   23.0   \n",
       "2017-02-28 22:00:00         21.0  10.5      11.0   7.8     10.0   7.8   23.0   \n",
       "2017-02-28 23:00:00         19.0   8.6      20.0   7.0     13.0   7.0   30.0   \n",
       "\n",
       "nodes                     Guanyuan        ... Nongzhanguan       Shunyi        \\\n",
       "channels                1        0     1  ...            0     1      0     1   \n",
       "datetime                                  ...                                   \n",
       "2013-03-01 00:00:00  -0.5      4.0  -0.7  ...          5.0  -0.5    3.0  -0.9   \n",
       "2013-03-01 01:00:00  -0.7      4.0  -1.1  ...          8.0  -0.7   12.0  -1.1   \n",
       "2013-03-01 02:00:00  -1.2      3.0  -1.1  ...          3.0  -1.2   14.0  -1.7   \n",
       "2013-03-01 03:00:00  -1.4      3.0  -1.4  ...          5.0  -1.4   12.0  -2.1   \n",
       "2013-03-01 04:00:00  -1.9      3.0  -2.0  ...          5.0  -1.9   12.0  -2.4   \n",
       "...                   ...      ...   ...  ...          ...   ...    ...   ...   \n",
       "2017-02-28 19:00:00  12.5     13.0  12.5  ...         14.0  12.5   27.0  10.3   \n",
       "2017-02-28 20:00:00  11.6     20.0  11.6  ...         18.0  11.6   47.0   9.8   \n",
       "2017-02-28 21:00:00  10.8     16.0  10.8  ...         15.0  10.8   18.0   9.1   \n",
       "2017-02-28 22:00:00  10.5     11.0  10.5  ...         11.0  10.5   18.0   7.1   \n",
       "2017-02-28 23:00:00   8.6     15.0   8.6  ...         10.0   8.6   15.0   7.4   \n",
       "\n",
       "nodes               Tiantan       Wanliu       Wanshouxigong        \n",
       "channels                  0     1      0     1             0     1  \n",
       "datetime                                                            \n",
       "2013-03-01 00:00:00     6.0  -0.5    8.0  -0.7           9.0   0.3  \n",
       "2013-03-01 01:00:00     6.0  -0.7    9.0  -1.1          11.0  -0.1  \n",
       "2013-03-01 02:00:00     6.0  -1.2    3.0  -1.1           8.0  -0.6  \n",
       "2013-03-01 03:00:00     6.0  -1.4   11.0  -1.4           8.0  -0.7  \n",
       "2013-03-01 04:00:00     5.0  -1.9    3.0  -2.0           8.0  -0.9  \n",
       "...                     ...   ...    ...   ...           ...   ...  \n",
       "2017-02-28 19:00:00    20.0  12.5   11.0  12.6          11.0  12.5  \n",
       "2017-02-28 20:00:00    11.0  11.6   15.0   9.4          13.0  11.6  \n",
       "2017-02-28 21:00:00    18.0  10.8   13.0   8.7          14.0  10.8  \n",
       "2017-02-28 22:00:00    15.0  10.5   12.0   7.8          12.0  10.5  \n",
       "2017-02-28 23:00:00    15.0   8.6    7.0   7.0          13.0   8.6  \n",
       "\n",
       "[35064 rows x 24 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tsl.data.datamodule.splitters import Splitter, disjoint_months\n",
    "from tsl.data.synch_mode import HORIZON\n",
    "from tsl.datasets.prototypes import DatetimeDataset\n",
    "from tsl.datasets.prototypes.mixin import MissingValuesMixin\n",
    "from tsl.utils import download_url, extract_zip\n",
    "\n",
    "\n",
    "def infer_mask(df, infer_from='next'):\n",
    "    \"\"\"Infer evaluation mask from DataFrame. In the evaluation mask a value is 1\n",
    "    if it is present in the DataFrame and absent in the :obj:`infer_from` month.\n",
    "\n",
    "    Args:\n",
    "        df (pd.Dataframe): The DataFrame.\n",
    "        infer_from (str): Denotes from which month the evaluation value must be\n",
    "            inferred. Can be either :obj:`previous` or :obj:`next`.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The evaluation mask for the DataFrame.\n",
    "    \"\"\"\n",
    "    mask = (~df.isna()).astype('uint8')\n",
    "    eval_mask = pd.DataFrame(index=mask.index, columns=mask.columns,\n",
    "                             data=0).astype('uint8')\n",
    "    if infer_from == 'previous':\n",
    "        offset = -1\n",
    "    elif infer_from == 'next':\n",
    "        offset = 1\n",
    "    else:\n",
    "        raise ValueError('`infer_from` can only be one of {}'.format(\n",
    "            ['previous', 'next']))\n",
    "    months = sorted(set(zip(mask.index.year, mask.index.month)))\n",
    "    length = len(months)\n",
    "    for i in range(length):\n",
    "        j = (i + offset) % length\n",
    "        year_i, month_i = months[i]\n",
    "        year_j, month_j = months[j]\n",
    "        cond_j = (mask.index.year == year_j) & (mask.index.month == month_j)\n",
    "        mask_j = mask[cond_j]\n",
    "        offset_i = 12 * (year_i - year_j) + (month_i - month_j)\n",
    "        mask_i = mask_j.shift(1, pd.DateOffset(months=offset_i))\n",
    "        mask_i = mask_i[~mask_i.index.duplicated(keep='first')]\n",
    "        mask_i = mask_i[np.in1d(mask_i.index, mask.index)]\n",
    "        i_idx = mask_i.index\n",
    "        eval_mask.loc[i_idx] = ~mask_i.loc[i_idx] & mask.loc[i_idx]\n",
    "    return eval_mask\n",
    "\n",
    "\n",
    "class AirQualitySplitter(Splitter):\n",
    "\n",
    "    def __init__(self,\n",
    "                 val_len: int = None,\n",
    "                 test_months: Sequence = (3, 6, 9, 12)):\n",
    "        super(AirQualitySplitter, self).__init__()\n",
    "        self._val_len = val_len\n",
    "        self.test_months = test_months\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        nontest_idxs, test_idxs = disjoint_months(dataset,\n",
    "                                                  months=self.test_months,\n",
    "                                                  synch_mode=HORIZON)\n",
    "        # take equal number of samples before each month of testing\n",
    "        val_len = self._val_len\n",
    "        if val_len < 1:\n",
    "            val_len = int(val_len * len(nontest_idxs))\n",
    "        val_len = val_len // len(self.test_months)\n",
    "        # get indices of first day of each testing month\n",
    "        delta = np.diff(test_idxs)\n",
    "        delta_idxs = np.flatnonzero(delta > delta.min())\n",
    "        end_month_idxs = test_idxs[1:][delta_idxs]\n",
    "        if len(end_month_idxs) < len(self.test_months):\n",
    "            end_month_idxs = np.insert(end_month_idxs, 0, test_idxs[0])\n",
    "        # expand month indices\n",
    "        month_val_idxs = [\n",
    "            np.arange(v_idx - val_len, v_idx) - dataset.window\n",
    "            for v_idx in end_month_idxs\n",
    "        ]\n",
    "        val_idxs = np.concatenate(month_val_idxs) % len(dataset)\n",
    "        # remove overlapping indices from training set\n",
    "        ovl_idxs, _ = dataset.overlapping_indices(nontest_idxs,\n",
    "                                                  val_idxs,\n",
    "                                                  synch_mode=HORIZON,\n",
    "                                                  as_mask=True)\n",
    "        train_idxs = nontest_idxs[~ovl_idxs]\n",
    "        self.set_indices(train_idxs, val_idxs, test_idxs)\n",
    "\n",
    "\n",
    "class AirQualitySmaller(DatetimeDataset, MissingValuesMixin):\n",
    "    similarity_options = {'distance'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = None,\n",
    "                 impute_nans: bool = True,\n",
    "                 test_months: Sequence = (3, 6, 9, 12),\n",
    "                 infer_eval_from: str = 'next',\n",
    "                 features: list = ['PM2.5'],\n",
    "                 freq: Optional[str] = None,\n",
    "                 masked_sensors: Optional[Sequence] = None):\n",
    "        self.root = root\n",
    "        self.test_months = test_months\n",
    "        self.infer_eval_from = infer_eval_from  # [next, previous]\n",
    "        self.features = features\n",
    "        if masked_sensors is None:\n",
    "            self.masked_sensors = []\n",
    "        else:\n",
    "            self.masked_sensors = list(masked_sensors)\n",
    "        df, mask, eval_mask, dist = self.load(impute_nans=impute_nans)\n",
    "        super().__init__(target=df,\n",
    "                         mask=mask,\n",
    "                         freq=freq,\n",
    "                         similarity_score='distance',\n",
    "                         temporal_aggregation='mean',\n",
    "                         spatial_aggregation='mean',\n",
    "                         default_splitting_method='air_quality',\n",
    "                         name='AQI12')\n",
    "        \n",
    "        self.add_covariate('dist', dist, pattern='n n')\n",
    "        self.set_eval_mask(eval_mask)\n",
    "\n",
    "        self.df = df\n",
    "        self.mask = mask\n",
    "        self.eval_masks = eval_mask\n",
    "        self.distance = dist\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['merged_full.csv']\n",
    "\n",
    "    @property\n",
    "    def required_file_names(self) -> List[str]:\n",
    "        return self.raw_file_names + ['aqi_dist.npy']\n",
    "\n",
    "    def build(self):\n",
    "        # compute distances from latitude and longitude degrees\n",
    "        path = os.path.join(self.root_dir, 'merged_full.csv')\n",
    "        stations = pd.DataFrame(pd.read_csv(path))\n",
    "        stations = stations.drop_duplicates(subset=[\"station\"])[[\"station\", \"locationLatitude\", \"locationLongitude\"]]\n",
    "        st_coord = stations.loc[:, ['locationLatitude', 'locationLongitude']]\n",
    "        from tsl.ops.similarities import geographical_distance\n",
    "        dist = geographical_distance(st_coord, to_rad=True).values\n",
    "        np.save(os.path.join(self.root_dir, 'aqi_dist.npy'), dist)\n",
    "\n",
    "    def load_raw(self):\n",
    "        self.maybe_build()\n",
    "        dist = np.load(os.path.join(self.root_dir, 'aqi_dist.npy'))\n",
    "        path = os.path.join(self.root_dir, 'merged_full.csv')\n",
    "        eval_mask = None\n",
    "        df = pd.read_csv(path)\n",
    "        df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "        df_pivot = df.pivot(index=\"datetime\", columns=\"station\", values=self.features)\n",
    "        df_pivot.columns.names = [\"channels\", \"nodes\"]\n",
    "        df_pivot.columns = df_pivot.columns.swaplevel(0, 1) \n",
    "        df_pivot.sort_index(axis=1, level=0, inplace=True)\n",
    "        df_pivot = df_pivot.rename(columns={feat: ind for ind, feat in enumerate(self.features)})\n",
    "        return pd.DataFrame(df_pivot), dist, eval_mask\n",
    "\n",
    "    def load(self, impute_nans=True):\n",
    "        # load readings and stations metadata\n",
    "        df, dist, eval_mask = self.load_raw()\n",
    "        # compute the masks:\n",
    "        mask = (~np.isnan(df.values)).astype('uint8')  # 1 if value is valid\n",
    "        if eval_mask is None:\n",
    "            eval_mask = infer_mask(df, infer_from=self.infer_eval_from)\n",
    "        # 1 if value is ground-truth for imputation\n",
    "        eval_mask = eval_mask.values.astype('uint8')\n",
    "        if len(self.masked_sensors):\n",
    "            eval_mask[:, self.masked_sensors] = mask[:, self.masked_sensors]\n",
    "        # eventually replace nans with weekly mean by hour\n",
    "        if impute_nans:\n",
    "            from tsl.ops.framearray import temporal_mean\n",
    "            df = df.fillna(temporal_mean(df))\n",
    "        return df, mask, eval_mask, dist\n",
    "\n",
    "    def get_splitter(self, method: Optional[str] = None, **kwargs):\n",
    "        if method == 'air_quality':\n",
    "            val_len = kwargs.get('val_len')\n",
    "            return AirQualitySplitter(test_months=self.test_months,\n",
    "                                      val_len=val_len)\n",
    "\n",
    "    def compute_similarity(self, method: str, **kwargs):\n",
    "        if method == \"distance\":\n",
    "            from tsl.ops.similarities import gaussian_kernel\n",
    "\n",
    "            # use same theta for both air and air36\n",
    "            theta = np.std(self.dist)\n",
    "            return gaussian_kernel(self.dist, theta=theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AirQualitySmaller('../../../AirData/AQI/Stations', impute_nans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35064, 12, 1), (8759, 36, 1))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.eval_mask.shape, dataset.eval_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "niwa_df = pd.read_csv('../../../AirData/Niwa/allNIWA_clarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "COLS = ['pm10ConcNumIndividual.value', 'pm1ConcNumIndividual.value',\n",
    "        'pm2_5ConcNumIndividual.value', 'relHumidInternalIndividual.value']\n",
    "AUCKLAND = {\n",
    "    'df' :      pd.DataFrame({\n",
    "                'locationLatitude': [-36.844079, -36.844113, -36.711932, -36.898491, -36.906652, -36.876728],\n",
    "                'locationLongitude': [174.762123, 174.761371, 174.740808, 174.591428, 174.633079, 174.703081]}), \n",
    "    'timezone': 'Pacific/Auckland'}\n",
    "\n",
    "def AirQualityCreate(path, features=None, t_range=None):\n",
    "    fin_cols = [COLS[i] for i in features]\n",
    "    features = {feat:'mean' for feat in fin_cols}\n",
    "\n",
    "    lat_long_vals = AUCKLAND[\"df\"]\n",
    "    time_zone = AUCKLAND['timezone']\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df['datetime'] = pd.to_datetime(df['time'], utc=True)\n",
    "    df['locationLatitude'] = df['locationLatitude'].round(6)\n",
    "    df['locationLongitude'] = df['locationLongitude'].round(6)\n",
    "    cols_to_keep = ['datetime', 'locationLatitude', 'locationLongitude'] + list(features.keys())\n",
    "\n",
    "    # Clean dataset\n",
    "    if features:\n",
    "        df = df[cols_to_keep]\n",
    "    if t_range:\n",
    "        df = df[(df['datetime'] > pd.to_datetime(t_range[0],unit=\"ns\", utc=True)) \n",
    "                & (df['datetime'] < pd.to_datetime(t_range[1],unit=\"ns\", utc=True))]\n",
    "    if not lat_long_vals.empty:\n",
    "        df = df.merge(lat_long_vals, on=['locationLatitude', 'locationLongitude'])\n",
    "\n",
    "    fin_df = df.groupby([pd.Grouper(key='datetime', freq='h'), 'locationLatitude', 'locationLongitude']).agg(features).reset_index()\n",
    "\n",
    "    unique_stations = fin_df[['locationLatitude', 'locationLongitude']].drop_duplicates().dropna().reset_index(drop=True)\n",
    "    unique_stations['station_id'] = range(1, len(unique_stations) + 1)  \n",
    "    \n",
    "    fin_df = fin_df.merge(unique_stations, on=['locationLatitude', 'locationLongitude'], how='left')\n",
    "\n",
    "    # Shape daset\n",
    "    unique_datetimes = fin_df[\"datetime\"].unique()\n",
    "\n",
    "    datetime_range = pd.date_range(start=np.min(unique_datetimes), end=np.max(unique_datetimes), freq='h')\n",
    "    unique_stations = fin_df[\"station_id\"].unique()\n",
    "\n",
    "    all_combinations = pd.DataFrame(\n",
    "        list(itertools.product(datetime_range, unique_stations)),\n",
    "        columns=[\"datetime\", \"station_id\"]\n",
    "    )\n",
    "\n",
    "    df_complete = all_combinations.merge(fin_df, on=[\"datetime\", \"station_id\"], how=\"left\")\n",
    "    df_complete[['locationLatitude', 'locationLongitude']] = \\\n",
    "        df_complete.groupby('station_id')[['locationLatitude', 'locationLongitude']].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    return df_complete\n",
    "\n",
    "niwa_df = AirQualityCreate('../../../AirData/Niwa/allNIWA_clarity.csv', [1, 2], ['2022-04-01', '2022-12-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station_id</th>\n",
       "      <th>locationLatitude</th>\n",
       "      <th>locationLongitude</th>\n",
       "      <th>pm1ConcNumIndividual.value</th>\n",
       "      <th>pm2_5ConcNumIndividual.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.906652</td>\n",
       "      <td>174.633079</td>\n",
       "      <td>3.036667</td>\n",
       "      <td>3.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-36.844113</td>\n",
       "      <td>174.761371</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>2.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>-36.844079</td>\n",
       "      <td>174.762123</td>\n",
       "      <td>3.363333</td>\n",
       "      <td>3.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-36.711932</td>\n",
       "      <td>174.740808</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>7.001429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>-36.898491</td>\n",
       "      <td>174.591428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35131</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-36.844113</td>\n",
       "      <td>174.761371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35132</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>-36.844079</td>\n",
       "      <td>174.762123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35133</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-36.711932</td>\n",
       "      <td>174.740808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35134</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>-36.898491</td>\n",
       "      <td>174.591428</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>3.803333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35135</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-36.876728</td>\n",
       "      <td>174.703081</td>\n",
       "      <td>25.963333</td>\n",
       "      <td>26.903333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       datetime  station_id  locationLatitude  \\\n",
       "0     2022-04-01 00:00:00+00:00           1        -36.906652   \n",
       "1     2022-04-01 00:00:00+00:00           2        -36.844113   \n",
       "2     2022-04-01 00:00:00+00:00           3        -36.844079   \n",
       "3     2022-04-01 00:00:00+00:00           4        -36.711932   \n",
       "4     2022-04-01 00:00:00+00:00           5        -36.898491   \n",
       "...                         ...         ...               ...   \n",
       "35131 2022-11-30 23:00:00+00:00           2        -36.844113   \n",
       "35132 2022-11-30 23:00:00+00:00           3        -36.844079   \n",
       "35133 2022-11-30 23:00:00+00:00           4        -36.711932   \n",
       "35134 2022-11-30 23:00:00+00:00           5        -36.898491   \n",
       "35135 2022-11-30 23:00:00+00:00           6        -36.876728   \n",
       "\n",
       "       locationLongitude  pm1ConcNumIndividual.value  \\\n",
       "0             174.633079                    3.036667   \n",
       "1             174.761371                    2.616667   \n",
       "2             174.762123                    3.363333   \n",
       "3             174.740808                    6.857143   \n",
       "4             174.591428                         NaN   \n",
       "...                  ...                         ...   \n",
       "35131         174.761371                         NaN   \n",
       "35132         174.762123                         NaN   \n",
       "35133         174.740808                         NaN   \n",
       "35134         174.591428                    3.550000   \n",
       "35135         174.703081                   25.963333   \n",
       "\n",
       "       pm2_5ConcNumIndividual.value  \n",
       "0                          3.100000  \n",
       "1                          2.673333  \n",
       "2                          3.410000  \n",
       "3                          7.001429  \n",
       "4                               NaN  \n",
       "...                             ...  \n",
       "35131                           NaN  \n",
       "35132                           NaN  \n",
       "35133                           NaN  \n",
       "35134                      3.803333  \n",
       "35135                     26.903333  \n",
       "\n",
       "[35136 rows x 6 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niwa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = niwa_df.pivot(index=\"datetime\", columns=\"station_id\", values=[COLS[i] for i in [1, 2]])\n",
    "df_pivot.columns.names = [\"channels\", \"nodes\"]\n",
    "df_pivot.columns = df_pivot.columns.swaplevel(0, 1) \n",
    "df_pivot.sort_index(axis=1, level=0, inplace=True)\n",
    "df_pivot = df_pivot.rename(columns={feat: ind for ind, feat in enumerate([COLS[i] for i in [1, 2]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th colspan=\"2\" halign=\"left\">1</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">3</th>\n",
       "      <th colspan=\"2\" halign=\"left\">4</th>\n",
       "      <th colspan=\"2\" halign=\"left\">5</th>\n",
       "      <th colspan=\"2\" halign=\"left\">6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-01 00:00:00+00:00</th>\n",
       "      <td>3.036667</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>2.616667</td>\n",
       "      <td>2.673333</td>\n",
       "      <td>3.363333</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>6.857143</td>\n",
       "      <td>7.001429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 01:00:00+00:00</th>\n",
       "      <td>3.117500</td>\n",
       "      <td>3.182500</td>\n",
       "      <td>2.635000</td>\n",
       "      <td>2.705000</td>\n",
       "      <td>3.072500</td>\n",
       "      <td>3.127500</td>\n",
       "      <td>6.986667</td>\n",
       "      <td>7.156667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 02:00:00+00:00</th>\n",
       "      <td>2.953333</td>\n",
       "      <td>3.036667</td>\n",
       "      <td>2.350000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>5.152500</td>\n",
       "      <td>5.253750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 03:00:00+00:00</th>\n",
       "      <td>3.480000</td>\n",
       "      <td>3.560000</td>\n",
       "      <td>2.320000</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>2.977500</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>4.665000</td>\n",
       "      <td>4.761667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 04:00:00+00:00</th>\n",
       "      <td>3.676667</td>\n",
       "      <td>3.796667</td>\n",
       "      <td>2.707500</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>3.203333</td>\n",
       "      <td>3.263333</td>\n",
       "      <td>4.172500</td>\n",
       "      <td>4.273750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 19:00:00+00:00</th>\n",
       "      <td>4.142500</td>\n",
       "      <td>4.507500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.296667</td>\n",
       "      <td>4.593333</td>\n",
       "      <td>30.396667</td>\n",
       "      <td>31.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 20:00:00+00:00</th>\n",
       "      <td>3.516667</td>\n",
       "      <td>3.830000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.100000</td>\n",
       "      <td>4.425000</td>\n",
       "      <td>29.887500</td>\n",
       "      <td>31.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 21:00:00+00:00</th>\n",
       "      <td>3.435000</td>\n",
       "      <td>3.740000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.170000</td>\n",
       "      <td>4.486667</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>31.803333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 22:00:00+00:00</th>\n",
       "      <td>3.306667</td>\n",
       "      <td>3.623333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.930000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>28.602500</td>\n",
       "      <td>29.595000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 23:00:00+00:00</th>\n",
       "      <td>3.367500</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>3.803333</td>\n",
       "      <td>25.963333</td>\n",
       "      <td>26.903333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5856 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes                             1                   2                   3  \\\n",
       "channels                          0         1         0         1         0   \n",
       "datetime                                                                      \n",
       "2022-04-01 00:00:00+00:00  3.036667  3.100000  2.616667  2.673333  3.363333   \n",
       "2022-04-01 01:00:00+00:00  3.117500  3.182500  2.635000  2.705000  3.072500   \n",
       "2022-04-01 02:00:00+00:00  2.953333  3.036667  2.350000  2.400000  3.066667   \n",
       "2022-04-01 03:00:00+00:00  3.480000  3.560000  2.320000  2.370000  2.977500   \n",
       "2022-04-01 04:00:00+00:00  3.676667  3.796667  2.707500  2.780000  3.203333   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "2022-11-30 19:00:00+00:00  4.142500  4.507500       NaN       NaN       NaN   \n",
       "2022-11-30 20:00:00+00:00  3.516667  3.830000       NaN       NaN       NaN   \n",
       "2022-11-30 21:00:00+00:00  3.435000  3.740000       NaN       NaN       NaN   \n",
       "2022-11-30 22:00:00+00:00  3.306667  3.623333       NaN       NaN       NaN   \n",
       "2022-11-30 23:00:00+00:00  3.367500  3.610000       NaN       NaN       NaN   \n",
       "\n",
       "nodes                                       4                   5            \\\n",
       "channels                          1         0         1         0         1   \n",
       "datetime                                                                      \n",
       "2022-04-01 00:00:00+00:00  3.410000  6.857143  7.001429       NaN       NaN   \n",
       "2022-04-01 01:00:00+00:00  3.127500  6.986667  7.156667       NaN       NaN   \n",
       "2022-04-01 02:00:00+00:00  3.140000  5.152500  5.253750       NaN       NaN   \n",
       "2022-04-01 03:00:00+00:00  3.030000  4.665000  4.761667       NaN       NaN   \n",
       "2022-04-01 04:00:00+00:00  3.263333  4.172500  4.273750       NaN       NaN   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "2022-11-30 19:00:00+00:00       NaN       NaN       NaN  4.296667  4.593333   \n",
       "2022-11-30 20:00:00+00:00       NaN       NaN       NaN  4.100000  4.425000   \n",
       "2022-11-30 21:00:00+00:00       NaN       NaN       NaN  4.170000  4.486667   \n",
       "2022-11-30 22:00:00+00:00       NaN       NaN       NaN  3.930000  4.220000   \n",
       "2022-11-30 23:00:00+00:00       NaN       NaN       NaN  3.550000  3.803333   \n",
       "\n",
       "nodes                              6             \n",
       "channels                           0          1  \n",
       "datetime                                         \n",
       "2022-04-01 00:00:00+00:00        NaN        NaN  \n",
       "2022-04-01 01:00:00+00:00        NaN        NaN  \n",
       "2022-04-01 02:00:00+00:00        NaN        NaN  \n",
       "2022-04-01 03:00:00+00:00        NaN        NaN  \n",
       "2022-04-01 04:00:00+00:00        NaN        NaN  \n",
       "...                              ...        ...  \n",
       "2022-11-30 19:00:00+00:00  30.396667  31.510000  \n",
       "2022-11-30 20:00:00+00:00  29.887500  31.017500  \n",
       "2022-11-30 21:00:00+00:00  30.740000  31.803333  \n",
       "2022-11-30 22:00:00+00:00  28.602500  29.595000  \n",
       "2022-11-30 23:00:00+00:00  25.963333  26.903333  \n",
       "\n",
       "[5856 rows x 12 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "COLS = ['pm10ConcNumIndividual.value', 'pm1ConcNumIndividual.value',\n",
    "        'pm2_5ConcNumIndividual.value', 'relHumidInternalIndividual.value']\n",
    "AUCKLAND = {\n",
    "    'df' :      pd.DataFrame({\n",
    "                'locationLatitude': [-36.844079, -36.844113, -36.711932, -36.898491, -36.906652, -36.876728],\n",
    "                'locationLongitude': [174.762123, 174.761371, 174.740808, 174.591428, 174.633079, 174.703081]}), \n",
    "    'timezone': 'Pacific/Auckland'}\n",
    "\n",
    "def AirQualityCreate(path, features=None, t_range=None):\n",
    "    for feat in features:\n",
    "        assert feat in COLS\n",
    "    \n",
    "    features = {feat:'mean' for feat in features}\n",
    "\n",
    "    lat_long_vals = AUCKLAND[\"df\"]\n",
    "    time_zone = AUCKLAND['timezone']\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df['datetime'] = pd.to_datetime(df['time'], utc=True)\n",
    "    df['locationLatitude'] = df['locationLatitude'].round(6)\n",
    "    df['locationLongitude'] = df['locationLongitude'].round(6)\n",
    "    cols_to_keep = ['datetime', 'locationLatitude', 'locationLongitude'] + list(features.keys())\n",
    "\n",
    "    # Clean dataset\n",
    "    if features:\n",
    "        df = df[cols_to_keep]\n",
    "    if t_range:\n",
    "        df = df[(df['datetime'] > pd.to_datetime(t_range[0],unit=\"ns\", utc=True)) \n",
    "                & (df['datetime'] < pd.to_datetime(t_range[1],unit=\"ns\", utc=True))]\n",
    "    if not lat_long_vals.empty:\n",
    "        df = df.merge(lat_long_vals, on=['locationLatitude', 'locationLongitude'])\n",
    "\n",
    "    fin_df = df.groupby([pd.Grouper(key='datetime', freq='h'), 'locationLatitude', 'locationLongitude']).agg(features).reset_index()\n",
    "\n",
    "    unique_stations = fin_df[['locationLatitude', 'locationLongitude']].drop_duplicates().dropna().reset_index(drop=True)\n",
    "    unique_stations['station'] = range(1, len(unique_stations) + 1)  \n",
    "    \n",
    "    fin_df = fin_df.merge(unique_stations, on=['locationLatitude', 'locationLongitude'], how='left')\n",
    "\n",
    "    # Shape daset\n",
    "    unique_datetimes = fin_df[\"datetime\"].unique()\n",
    "\n",
    "    datetime_range = pd.date_range(start=np.min(unique_datetimes), end=np.max(unique_datetimes), freq='h')\n",
    "    unique_stations = fin_df[\"station\"].unique()\n",
    "\n",
    "    all_combinations = pd.DataFrame(\n",
    "        list(itertools.product(datetime_range, unique_stations)),\n",
    "        columns=[\"datetime\", \"station\"]\n",
    "    )\n",
    "\n",
    "    df_complete = all_combinations.merge(fin_df, on=[\"datetime\", \"station\"], how=\"left\")\n",
    "    df_complete[['locationLatitude', 'locationLongitude']] = \\\n",
    "        df_complete.groupby('station')[['locationLatitude', 'locationLongitude']].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    return df_complete\n",
    "\n",
    "niwa_df = AirQualityCreate('../../../AirData/Niwa/allNIWA_clarity.csv', ['pm2_5ConcNumIndividual.value', 'relHumidInternalIndividual.value'], ['2022-04-01', '2022-12-01'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>station</th>\n",
       "      <th>locationLatitude</th>\n",
       "      <th>locationLongitude</th>\n",
       "      <th>pm2_5ConcNumIndividual.value</th>\n",
       "      <th>relHumidInternalIndividual.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>-36.906652</td>\n",
       "      <td>174.633079</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>57.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-36.844113</td>\n",
       "      <td>174.761371</td>\n",
       "      <td>2.673333</td>\n",
       "      <td>51.863333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>-36.844079</td>\n",
       "      <td>174.762123</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>55.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-36.711932</td>\n",
       "      <td>174.740808</td>\n",
       "      <td>7.001429</td>\n",
       "      <td>65.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-04-01 00:00:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>-36.898491</td>\n",
       "      <td>174.591428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35131</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>-36.844113</td>\n",
       "      <td>174.761371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35132</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>-36.844079</td>\n",
       "      <td>174.762123</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35133</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-36.711932</td>\n",
       "      <td>174.740808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35134</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>-36.898491</td>\n",
       "      <td>174.591428</td>\n",
       "      <td>3.803333</td>\n",
       "      <td>62.193333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35135</th>\n",
       "      <td>2022-11-30 23:00:00+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-36.876728</td>\n",
       "      <td>174.703081</td>\n",
       "      <td>26.903333</td>\n",
       "      <td>57.953333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35136 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       datetime  station  locationLatitude  locationLongitude  \\\n",
       "0     2022-04-01 00:00:00+00:00        1        -36.906652         174.633079   \n",
       "1     2022-04-01 00:00:00+00:00        2        -36.844113         174.761371   \n",
       "2     2022-04-01 00:00:00+00:00        3        -36.844079         174.762123   \n",
       "3     2022-04-01 00:00:00+00:00        4        -36.711932         174.740808   \n",
       "4     2022-04-01 00:00:00+00:00        5        -36.898491         174.591428   \n",
       "...                         ...      ...               ...                ...   \n",
       "35131 2022-11-30 23:00:00+00:00        2        -36.844113         174.761371   \n",
       "35132 2022-11-30 23:00:00+00:00        3        -36.844079         174.762123   \n",
       "35133 2022-11-30 23:00:00+00:00        4        -36.711932         174.740808   \n",
       "35134 2022-11-30 23:00:00+00:00        5        -36.898491         174.591428   \n",
       "35135 2022-11-30 23:00:00+00:00        6        -36.876728         174.703081   \n",
       "\n",
       "       pm2_5ConcNumIndividual.value  relHumidInternalIndividual.value  \n",
       "0                          3.100000                         57.160000  \n",
       "1                          2.673333                         51.863333  \n",
       "2                          3.410000                         55.910000  \n",
       "3                          7.001429                         65.714286  \n",
       "4                               NaN                               NaN  \n",
       "...                             ...                               ...  \n",
       "35131                           NaN                               NaN  \n",
       "35132                           NaN                               NaN  \n",
       "35133                           NaN                               NaN  \n",
       "35134                      3.803333                         62.193333  \n",
       "35135                     26.903333                         57.953333  \n",
       "\n",
       "[35136 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "niwa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirQualityAuckland(DatetimeDataset, MissingValuesMixin):\n",
    "    similarity_options = {'distance'}\n",
    "\n",
    "    def __init__(self,\n",
    "                 root: str = None,\n",
    "                 impute_nans: bool = True,\n",
    "                 test_months: Sequence = (9, 10, 11),\n",
    "                 infer_eval_from: str = 'next',\n",
    "                 features: list = ['pm2_5ConcNumIndividual.value'],\n",
    "                 t_range: Optional[list] = None,\n",
    "                 freq: Optional[str] = None,\n",
    "                 masked_sensors: Optional[Sequence] = None):\n",
    "        self.root = root\n",
    "        self.test_months = test_months\n",
    "        self.infer_eval_from = infer_eval_from  # [next, previous]\n",
    "        self.features = features\n",
    "        self.t_range = t_range\n",
    "\n",
    "        if masked_sensors is None:\n",
    "            self.masked_sensors = []\n",
    "        else:\n",
    "            self.masked_sensors = list(masked_sensors)\n",
    "        \n",
    "        df, mask, eval_mask, dist = self.load(impute_nans=impute_nans)\n",
    "        super().__init__(target=df,\n",
    "                         mask=mask,\n",
    "                         freq=freq,\n",
    "                         similarity_score='distance',\n",
    "                         temporal_aggregation='mean',\n",
    "                         spatial_aggregation='mean',\n",
    "                         default_splitting_method='air_quality',\n",
    "                         name='AQI12')\n",
    "        \n",
    "        self.add_covariate('dist', dist, pattern='n n')\n",
    "        self.set_eval_mask(eval_mask)\n",
    "\n",
    "        self.df = df\n",
    "        # self.mask = mask\n",
    "        # self.eval_masks = eval_mask\n",
    "        # self.distance = dist\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['allNIWA_clarity.csv']\n",
    "\n",
    "    @property\n",
    "    def required_file_names(self) -> List[str]:\n",
    "        return self.raw_file_names + ['auck_aqi_dist.npy']\n",
    "\n",
    "    def build(self):\n",
    "        # compute distances from latitude and longitude degrees\n",
    "        path = os.path.join(self.root_dir, 'allNIWA_clarity.csv')\n",
    "        stations = AirQualityCreate(path, self.features, self.t_range)\n",
    "        stations = stations.drop_duplicates(subset=[\"station\"])[[\"station\", \"locationLatitude\", \"locationLongitude\"]]\n",
    "        st_coord = stations.loc[:, ['locationLatitude', 'locationLongitude']]\n",
    "        from tsl.ops.similarities import geographical_distance\n",
    "        dist = geographical_distance(st_coord, to_rad=True).values\n",
    "        np.save(os.path.join(self.root_dir, 'auck_aqi_dist.npy'), dist)\n",
    "\n",
    "    def load_raw(self):\n",
    "        self.maybe_build()\n",
    "        dist = np.load(os.path.join(self.root_dir, 'auck_aqi_dist.npy'))\n",
    "        path = os.path.join(self.root_dir, 'allNIWA_clarity.csv')\n",
    "        eval_mask = None\n",
    "        df = AirQualityCreate(path, self.features, self.t_range)\n",
    "\n",
    "        df_pivot = df.pivot(index=\"datetime\", columns=\"station\", values=self.features)\n",
    "        df_pivot.columns.names = [\"channels\", \"nodes\"]\n",
    "        df_pivot.columns = df_pivot.columns.swaplevel(0, 1) \n",
    "        df_pivot.sort_index(axis=1, level=0, inplace=True)\n",
    "        df_pivot = df_pivot.rename(columns={feat: ind for ind, feat in enumerate(self.features)})\n",
    "        \n",
    "        return pd.DataFrame(df_pivot), dist, eval_mask\n",
    "\n",
    "    def load(self, impute_nans=True):\n",
    "        # load readings and stations metadata\n",
    "        df, dist, eval_mask = self.load_raw()\n",
    "        # compute the masks:\n",
    "        mask = (~np.isnan(df.values)).astype('uint8')  # 1 if value is valid\n",
    "        if eval_mask is None:\n",
    "            eval_mask = np.random.randint(2, size=mask.shape)\n",
    "            eval_mask *= mask\n",
    "        if len(self.masked_sensors):\n",
    "            mask[:, self.masked_sensors] = 0\n",
    "            eval_mask[:, self.masked_sensors] = 0\n",
    "        # eventually replace nans with weekly mean by hour\n",
    "        if impute_nans:\n",
    "            from tsl.ops.framearray import temporal_mean\n",
    "            df = df.fillna(temporal_mean(df))\n",
    "        return df, mask, eval_mask, dist\n",
    "\n",
    "    def get_splitter(self, method: Optional[str] = None, **kwargs):\n",
    "        if method == 'air_quality':\n",
    "            val_len = kwargs.get('val_len')\n",
    "            return AirQualitySplitter(test_months=self.test_months,\n",
    "                                      val_len=val_len)\n",
    "\n",
    "    def compute_similarity(self, method: str, **kwargs):\n",
    "        if method == \"distance\":\n",
    "            from tsl.ops.similarities import gaussian_kernel\n",
    "\n",
    "            # use same theta for both air and air36\n",
    "            theta = np.std(self.dist)\n",
    "            return gaussian_kernel(self.dist, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AirQualityAuckland('../../../AirData/Niwa/', t_range=['2022-04-01', '2022-12-01'], masked_sensors=[2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]]]),\n",
       " array([[[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]],\n",
       " \n",
       "        [[False],\n",
       "         [False]]]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.eval_mask[:, [2,3]], dataset.mask[:, [2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]],\n",
       "\n",
       "       [[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        [False],\n",
       "        [False]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True]],\n",
       "\n",
       "       [[ True],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [ True],\n",
       "        [ True]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = {'u': dataset.datetime_encoded('day').values}\n",
    "\n",
    "# get adjacency matrix\n",
    "adj = dataset.get_connectivity(method = 'distance', threshold = 1e-4, include_self= False)\n",
    "# u = np.expand_dims(covariates['u'], axis=1)\n",
    "# covariates['u'] = np.repeat(u, max(adj[0]), axis=1)\n",
    "# print(covariates['u'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset = ImputationDataset(target=dataset.dataframe(),\n",
    "                                    mask=dataset.training_mask,\n",
    "                                    eval_mask=dataset.eval_mask,\n",
    "                                    covariates=covariates,\n",
    "                                    transform=MaskInput(),\n",
    "                                    connectivity=adj)\n",
    "\n",
    "scalers = {'target': StandardScaler(axis=(0, 1))}\n",
    "\n",
    "dm = SpatioTemporalDataModule(\n",
    "    dataset=torch_dataset,\n",
    "    scalers=scalers,\n",
    "    splitter=dataset.get_splitter(val_len= 0.1, test_len= 0.2),\n",
    "    batch_size=34,\n",
    "    workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup(stage='fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3672, 3673, 3674, ..., 5842, 5843, 5844])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.testset.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9608"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(dataset.eval_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>nodes</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>channels</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-01 00:00:00+00:00</th>\n",
       "      <td>3.100000</td>\n",
       "      <td>2.673333</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>7.001429</td>\n",
       "      <td>2.084038</td>\n",
       "      <td>26.935846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 01:00:00+00:00</th>\n",
       "      <td>3.182500</td>\n",
       "      <td>2.705000</td>\n",
       "      <td>3.127500</td>\n",
       "      <td>7.156667</td>\n",
       "      <td>1.952051</td>\n",
       "      <td>26.430929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 02:00:00+00:00</th>\n",
       "      <td>3.036667</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>5.253750</td>\n",
       "      <td>1.909407</td>\n",
       "      <td>26.247177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 03:00:00+00:00</th>\n",
       "      <td>3.560000</td>\n",
       "      <td>2.370000</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>4.761667</td>\n",
       "      <td>2.000657</td>\n",
       "      <td>25.495037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01 04:00:00+00:00</th>\n",
       "      <td>3.796667</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>3.263333</td>\n",
       "      <td>4.273750</td>\n",
       "      <td>2.001064</td>\n",
       "      <td>22.854179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 19:00:00+00:00</th>\n",
       "      <td>4.507500</td>\n",
       "      <td>3.220572</td>\n",
       "      <td>6.007094</td>\n",
       "      <td>3.583112</td>\n",
       "      <td>4.593333</td>\n",
       "      <td>31.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 20:00:00+00:00</th>\n",
       "      <td>3.830000</td>\n",
       "      <td>3.538578</td>\n",
       "      <td>8.621893</td>\n",
       "      <td>4.527214</td>\n",
       "      <td>4.425000</td>\n",
       "      <td>31.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 21:00:00+00:00</th>\n",
       "      <td>3.740000</td>\n",
       "      <td>4.023368</td>\n",
       "      <td>7.992508</td>\n",
       "      <td>2.938905</td>\n",
       "      <td>4.486667</td>\n",
       "      <td>31.803333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 22:00:00+00:00</th>\n",
       "      <td>3.623333</td>\n",
       "      <td>3.346984</td>\n",
       "      <td>7.031816</td>\n",
       "      <td>2.417434</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>29.594999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-30 23:00:00+00:00</th>\n",
       "      <td>3.610000</td>\n",
       "      <td>3.259624</td>\n",
       "      <td>8.664185</td>\n",
       "      <td>2.690928</td>\n",
       "      <td>3.803333</td>\n",
       "      <td>26.903334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5856 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "nodes                             1         2         3         4         5  \\\n",
       "channels                          0         0         0         0         0   \n",
       "datetime                                                                      \n",
       "2022-04-01 00:00:00+00:00  3.100000  2.673333  3.410000  7.001429  2.084038   \n",
       "2022-04-01 01:00:00+00:00  3.182500  2.705000  3.127500  7.156667  1.952051   \n",
       "2022-04-01 02:00:00+00:00  3.036667  2.400000  3.140000  5.253750  1.909407   \n",
       "2022-04-01 03:00:00+00:00  3.560000  2.370000  3.030000  4.761667  2.000657   \n",
       "2022-04-01 04:00:00+00:00  3.796667  2.780000  3.263333  4.273750  2.001064   \n",
       "...                             ...       ...       ...       ...       ...   \n",
       "2022-11-30 19:00:00+00:00  4.507500  3.220572  6.007094  3.583112  4.593333   \n",
       "2022-11-30 20:00:00+00:00  3.830000  3.538578  8.621893  4.527214  4.425000   \n",
       "2022-11-30 21:00:00+00:00  3.740000  4.023368  7.992508  2.938905  4.486667   \n",
       "2022-11-30 22:00:00+00:00  3.623333  3.346984  7.031816  2.417434  4.220000   \n",
       "2022-11-30 23:00:00+00:00  3.610000  3.259624  8.664185  2.690928  3.803333   \n",
       "\n",
       "nodes                              6  \n",
       "channels                           0  \n",
       "datetime                              \n",
       "2022-04-01 00:00:00+00:00  26.935846  \n",
       "2022-04-01 01:00:00+00:00  26.430929  \n",
       "2022-04-01 02:00:00+00:00  26.247177  \n",
       "2022-04-01 03:00:00+00:00  25.495037  \n",
       "2022-04-01 04:00:00+00:00  22.854179  \n",
       "...                              ...  \n",
       "2022-11-30 19:00:00+00:00  31.510000  \n",
       "2022-11-30 20:00:00+00:00  31.017500  \n",
       "2022-11-30 21:00:00+00:00  31.803333  \n",
       "2022-11-30 22:00:00+00:00  29.594999  \n",
       "2022-11-30 23:00:00+00:00  26.903334  \n",
       "\n",
       "[5856 rows x 6 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6075"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = 0\n",
    "for ind in dm.trainset.indices:\n",
    "    sums += np.sum(dataset.eval_mask[ind, :])\n",
    "sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True],\n",
       "         [False],\n",
       "         [False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True]],\n",
       "\n",
       "        [[False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [False],\n",
       "         [ True]]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.torch_dataset.eval_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
